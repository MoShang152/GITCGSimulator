### Alpha-beta剪枝+多层次Razor+专家排序

alpha-beta剪枝的标准形式. 我们约定所有估计都是相对于牌手1而言的(绝对价值而非相对价值).

```py

def alpha_beta(s, alpha,beta,depth):
    if depth <= 0:
        return evaluate(s)
    for move in s.allmoves():
        next = s.proceed(move)
        val = -alpha_beta(next, -beta, -alpha, depth - 1)
        if val >= beta:
            ##触发beta剪枝: 此状态下存在一个过于好的着法, 而对方存在一个阻止你达到此状态的手段
            return beta
        if val > alpha:
            alpha = val##收紧下界
    return alpha

alpha_beta(start, -inf,+inf, DEPTH)
```

可以在代码中记载最好的val所对应的move直接指导下棋.

主要变例:双方的最佳应对路线. 在迭代加深中可以复用上一次的主要变例,优先搜索该变例.

#### 非典型回合交换AI

七圣中,可能我方行动后还是我方行动.

此时, 需要判断上下两层是否是同一个mover. 若是, 则原样传递参数,返回值也不取反; 否则, 参数取反, 返回值也要取反. 同时要注意的是, 评价函数val必须是相对于当前行动者而言的. (negmax要求)

#### 多层次Razor:

所谓Razor就是在当前局面$s$下, 收益是$v(s)$. 我们期望通过搜索来改进对于$s$的估值, 不妨假设搜索改进后为$v'(s)$. 那么搜索的收益就是

$$
\delta = v'(s) - v(s)
$$

我们经验地确定一个常数$R$, 使得对于大部分搜索行为而言,$\delta < R$, 那么$R$就代表了我们**期望**搜索的能力极限.

对于当前状态, 如果$v(s)+R<\alpha$, 说明这是一个**预期很不好**的节点, 对于该节点, 我们分配较小的搜索深度(一般是在当前深度上-1).

反映在代码中:

```py
##在搜索前插入
if evaluate(s) + R < alpha:
    depth = depth - 1

for move ....
```

$R$的选取是固定的,但我们更愿意让其随着搜索深度而变化. 当depth还剩余很多时, 搜索是有益的, 因此$R$应当大一些; 当depth接近耗尽时, 搜索能做的事情很少, $R$应当相对来说小一些.

> 通常在前几个搜索层不会引入任何裁剪算法. 在残局时也会减少裁剪, 以免漏算.


#### 自损裁剪

一个经典的裁剪手段叫做"空着裁剪". 它适用于棋类的很重要的因素在于对于大部分棋类, 连走两步总会带来优势; 然而这对七圣召唤并不适用. 七圣召唤只有在残局时连续行动才有优势, 因为七圣召唤的每次行动总会消耗资源, 因此过快地消耗资源并不总是好的.

然而,我们可以借鉴其思想: 如果我让自己强行亏损一部分再进行搜索(此时只进行浅层搜索,一般深度减2或者3), 如果搜出来的值仍然可以触发beta裁剪, 那么说明目前这个状态过于好了, 对方一定会阻止你达到这个状态的, 我们直接返回beta就好.

(如果没有, 则需要重新进行完全搜索)

那么什么时候进行自损裁剪呢? 显然, 如果当前局面预期加上搜索所要带来的改进$R$(如前所述)比beta还要更高, 那么我们就有信心认为当前局面足够好了, 便可以尝试自损裁剪. 换句话说, 如果

$$
V(s) + R > beta
$$

则触发自损裁剪. 此时对$s$执行一些操作(对应到七圣来说, 一般是去掉两个骰子. 这有很多好处, 不仅减少了搜索深度, 由于七圣的骰子会影响行动数, 这也会减少分支因子).

一个谨慎的程序可能使用$R/2$而非$R$.

自损裁剪相对于空着还有一个好处是, 我们可以连续尝试自损裁剪(只要条件能被触发).

> 在EasyCard的测试结果中, 自损裁剪+Razor在执自己认为弱势的一方时仍然击败了纯alpha-beta和Razor策略. 纯自损裁剪没能打赢纯alpha-beta. Razor打赢了纯alpha-beta.  Razor后手击败了混合算法. 可以看到后手比较有优势(天然少算一层). 
> 混合策略先手击败了纯alpha-beta(缩短时间). 

#### 概率裁剪

如果所有的裁剪都是硬边界,那么在相同情况下电脑只会选择同样的着法, 这一来容易被牌手观察利用, 二来如果该裁剪出错, AI永远无法发现这个问题. 因此, 对于所有裁剪, 如果它可能被触发, 那么它将以概率$p$被触发. 一般来说, 离硬边界越近, $p$越小(但也不应该低于`0.9`). 


#### 次序独立卡牌

所谓次序独立, 就是对于某张牌$C$而言, 对于其他任何手牌$H_i$, 序列$C,H_i$和序列$H_i,C$产生的效果是一样的. 强次序独立指的是当$H_i=C$时也成立的卡牌; 一般次序独立并不考虑连续打出同一张卡.

典型的次序独立卡牌是交给我吧和换班时间.

典型的非次序独立卡牌是 料理和场地. 另, 所有标记为战斗行动的卡牌都是非次序独立的.

在进行搜索时, 我们总是尝试先打出次序独立卡牌; 如果当前节点搜索的是非次序独立卡牌, 那么不会搜索它的 次序独立卡牌 子节点.

我们不使用哈希表去重的原因是卡牌如果生成出战状态, 不同顺序出牌会造成不同的出战状态, 这会导致哈希不一样(而其实本应是一样的). 如果哈希时忽略顺序, 又会导致非次序独立卡牌结算错误.

#### 专家排序

alpha-beta相比minimax可以优化到$O(\sqrt{N})$,而想要稳定地达到此级别则需要按最优着法进行:每一步总能最快地触发beta剪枝.

关于七圣, 着法排序大概存在以下规则:

```
调和后不会立刻切人(除非留立本骰子等特殊情况).

打出特定卡牌后倾向于执行特定行动(换班,速切==>切人)

某些角色重复释放技能很亏(各种召唤物角色).

开局第一次行动不会切人(除非骰子烂了)
```

换句话说七圣的大部分行动和上一个行动相关性很强, 可以据此排序.

注意, 排序会受到很多因素的影响. 首要因素是**主要变例**, 即已经搜索过的双方最佳应对. 通常主要变例只会影响一个行动(因为一方每次也就一个行动). 剩余行动的安排则由专家排序给出.

#### 窗口缩减

alpha-beta搜索本质上就是不断地缩减`[alpha,beta]`这个窗口的大小, 直至无法缩减(此时将采取alpha行动. 所以根节点本身是alpha节点). 

我们可以通过强制缩小该窗口让搜索更快地触发beta剪枝. 简单来说, 我们通过某种方法预期本次搜索的返回值范围, 然后将窗口尽可能地缩减(一般是0窗口,即`[x,x+1]`), 以便产生更多截断.

需要说明的是, 如果搜索返回x+1(返回上界),则需要重新正常搜索一遍. 此方法需要足够的知识先验地提出估计.

> 此方法在AI完善后可能加入, 初期不加入. 重复搜索会浪费计算资源.

## Monte-Carlo alphabeta

我们将可以引发"我方抽牌"或者"我方掷骰"(注意必须是我方,因为对方的一切信息都是未知的,我们用固定的猜测代替)的行动称为一个**机会行动**. 机会行动会进入一个**机会节点**. 机会节点包含一个概率事件, 所以机会节点可能转化成多种不同的实在节点. 为了准确测算机会节点的价值, 我们可以使用蒙特卡洛方法:

若本次行动是一个机会行动, 则采样$\mathcal{S}$次机会节点的价值, 根据平均数估计其价值. 为了降低算力开销, 我们会在进入机会节点后缩减深度(一般是$1$或者$2$).

具体到实现来说, 掷骰可能也会被固定期望所代替(掷骰的组合数太多, 少量蒙特卡洛的模拟效果不好). 然而抽卡是一定会被模拟的(具体而言,可以估算若我方抢先有多大概率拿到翻盘卡).

## 评价函数

评价函数非常重要: 无论多么强大的剪枝算法也有它的深度极限; 向前搜索十几步后便无能为力(对于分支较高的游戏, 向前搜索8步左右就陷入分支爆炸). 为了辨析各种相似局面之间的不同, 评价函数必须具有指导意义. 

### 平滑

最好的评价函数就是相邻的状态节点之间的评价差异不大;尤其是涉及到终结状态时.

在非终结节点时使用血量差作为评价函数, 而终结节点返回一个超大值, 这样的函数会造成严重的地平线效应; 如果没有搜索到终结节点, 那么就无从分辨差1和差2究竟会带来多大的区别.

最好的评价函数应该满足如下性质:

$$
|V(s')-V(s)| \leq \varepsilon, s'\in S(s)
$$

### 七圣召唤

对于七圣的评价函数而言, 需要满足以下两个硬约束:

$$
V(s_{2})=-inf, \text{我方hp<=0}
$$

$$
V(s_{1}) = +inf, \text{对方hp<=0}
$$

下标表示谁获胜了. 无论inf的具体数字是多少, 必须满足对称性$V(s_2)=-V(s_1)$

## rating

软件会对每一步打分.

对于当前行动者而言, 若他的某个行动$a$可以改进或者保持当前局面(可能允许一到两分的偏移)的评价, 则称$a$为正着; 若行动$a$并非正着, 但保持局面大于等于零, 则称$a$为缓手; 若$a$使得评价降低至0以下, 则称为错着.

将行动按照对于局面的改进分数进行排序, 分值从$10$到$0$, 各行动的分数按均匀分割$[0,10]$区间进行分配.

特别地, 若$a$是唯一正着, 它获得额外$10$分. 若$a$是pass且剩余超过$3$个及以上元素骰且不存在桓那兰纳(若存在, 则应当超过5个元素骰)或者立本, 则再获得$10$分. 若$a$是唯一错着, 它获得额外$-10$分.

评分会对选手有指导意义.

将各步分数进行平均, 可以估算本局的精彩程度.